\chapter{Background}
In human-human communication, human would be able to understand each other through multiply ways. Human is able to understand others by conversation, facial expression, gesture, even intonation, mood. This phonomenon was introduced into human-computer interactions(HCI). Using computer to automatically analyze human face, voice and behaviours would be helpful in imporving HCI. This techniques not only can be used in avoid impostor attacks but also be used in automatic speech recognition. Face and voice are important and personal biometric characteristics. Compare to traditional knowledge-based and token-based person recognition method, biometric recogntion technology are more convenient and safer. With the increasing in computer computing power and development in computer vision, Automatic Speech Recognition(ASR) attracts more attention in recent years, from traditional audio-only ASR to audiovisual ASR, a huge progress was made in ASR.

\section{Automatic Speeech Recognition}
Intuition of Automatic Speech Recognition is to convert a spoken sentence into to readable text in real time by computer. It has been researched over 50 years, and the ultimate goal of ASR is to let a computer $100\%$ accurately recognise speech of any person under any environment. However, the accurace of recognition highly depended on robust information channel, background environment, the training data base and the adaptation of speaker to database. The beneficial of ASR is quite obvious, it can help deaf people listening by convert speech into readable text and help hard-reading people on reading by convert readable text into voice. The search engine may not be limited on text searching but also speech search. With decreasing price of computing power, Speech recognition techniques are widely used on mobile devices, like siri.

As in human-human communication, visual feedback is very important, visual information plays an important role in helping people understanding each other. Visual modality was proved to have positive influence on reducing noise in ASR and the history can be quantified back to 1954 \cite{potamianos2003recent}. Deaf people is able to communicate with each other by reading mouth movements. \cite{potamianos2003recent} gave three key reasons to include vision information in human speech recognition. Firstly , it helps audio source localisation, visual information of tongue, teeth, and lips provide complementary information of articulation. It is beneficial for distinguish confusable acoustics such as unvoiced consonants $/p/$ by providing information of facial muscle movements. Facial muscle movements are robust information for ASR. This technique of using visual information to recognise speech is known as automatic lipreading or speechreading in ASR \cite{potamianos2003recent}.

Audiovisual ASR uses both visual modality and audio modality in recognizing speech. Two main challenges are introduced by AV-ASR, how to extract visual features, how to combine it with audio features. Visual speech information mainly from speaker's face. Extract visual feature requires face detection, face alignment, tracking, feature extraction and other techniques to extract useful visual information from image with a face. Combination of two modality are also a challenge for AV-ASR.
\section{Visual Front End}
A major problem in audiovisual automatic speech recognition is extracting visual feature from images, the inputs are usuablly videos and the output should be visual speech features. Generally, visual speech feature can be classified as three types: appearance feature, shape fewature, combination of both\cite{potamianos2003recent}. The image of Region of Interest used to be directly used for trainning and classification. However, image data contain many noises and influenced by the lighting condition. Then some techniques in computer vision are used to extract image features from Region of Interest. Image feature contains points, edges, texture, colors and so on. Shape feature usually means coutours of speaker's face, specifically speaker's lips or including jaw and cheek. Shape feature usually means geometric-type features, such as statistical shape model or image moment descriptor of mouth, these model would be able to contain the information of the height, width and other information of a mouth. Combined feature usually is the joint of both shape and appearance feature vectors or a model that include both features like active appearance model.

In order to get appearance feature and shape feature or combined feature, there are some preprocessing before extracting feature. Face detect, tracking, alignment, and ROI extracting technique are needed. In order to align a face with feature points, some statistical models are used to fit the face. In order to remove head-pose, some technique are used to decompsite the head points. If the head is not facing the front, apperance of face would not be correct, warping techniqure are also needed.
\subsection{Face Detection}
Several main aspects that influence face detection is background, head pose and lighting. \cite{potamianos2003recent} reports taht many system uses traditional image processing techniques, such as color segmentation, edge detection, image thresholding, template matching or motion information and some using statistical modeling and neural networks. Once a face is detected, use face alignment technique to locate several facial feature around the face.
\subsection{Region of Interest}
The choice of Region of Interest(ROI) is according to the purpose of project. In AV-ASR it usually include large part of the lower face, such as the jaw, and cheeks or even the entire face \cite{potamianos2003recent}, as when people speak, the lower face would show some movements. In my project, the ROI only contains the grayscale values of mouth region, which is scaled to $8 * 32$ size square region. \cite{potamianos2003recent} report that experiments shows that including jaw and cheeks was beneficial. As the tracker I use for tracking face does not include the jaw and cheek of the face, I have to just include the mouth.

\section{Facial Expression}

\section{Nonlinguistic Vocalization}