\chapter{Background}
In human-human communication, human would be able to understand each other through variety feedbacks. We are able to understand others through conversation, facial expression, gesture, even intonation, mood. This phenomenon was introduced into human-computer interactions(HCI). Using computer to automatically analyse human face, voice and behaviours would be helpful in improving HCI. Many technology are developed for computer to automatically recognize human actions and emotions such as automatic speech recognition\cite{potamianos2003recent}, automatic recognize facial expression\cite{xiong2013supervised}. These technologies not only can be used for HCI, but also can be used in other area such as avoid impostor attacks\cite{aleksic2006audio}. Face and voice are important and personal biometric characteristics. Compare to traditional knowledge-based and token-based person recognition method, biometric recognition technology are more convenient and safer. With the increasing in computer computing power and development in computer vision, Automatic Speech Recognition(ASR) attracts more attention in recent years, from traditional audio-only ASR to audiovisual ASR, a huge progress was made in ASR. AV-ASR could be used on automatic generation of voice and facial animation from arbitrary speech.

\section{Automatic Speech Recognition}
The intuition of Automatic Speech Recognition is to convert a spoken sentence into to readable text in real time by computer and he ultimate goal of ASR is to let a computer $100\%$ accurately recognise speech of any person under any environment. The beneficial of ASR is quite obvious, it can help deaf people listening by convert speech into readable text and help hard-reading people on reading by convert readable text into voice. The search engine may not be limited on text searching but also speech search. It has been studied for over 50 years, recent work has a great improvement in this area. The accuracy of recognition highly depended on robust information channel, background environment, the training data base and the adaptation of speaker to database. With decreasing price of computing power, Speech recognition techniques are widely used on mobile devices, like Siri.

People can not only know what others says by listening to speaker, they can also know what others says by watching. Deaf people is able to communicate with each other by reading mouth movements. Visual modality was proved to have positive influence on reducing noise in ASR and the history can be quantified back to 1954 \cite{potamianos2003recent}. \cite{potamianos2003recent} gave three key reasons to include vision information in human speech recognition. Firstly , it helps audio source localisation, visual information of tongue, teeth, and lips provide complementary information of articulation. Secondly, it is beneficial for distinguish confusable acoustics such as unvoiced consonants $/p/$ by providing information of facial muscle movements. Thirdly, facial muscle movements are robust information for ASR. This technique of using visual information to recognise speech is known as automatic lip-reading or speech-reading in ASR \cite{potamianos2003recent}.

Audiovisual ASR uses both visual modality and audio modality in recognizing speech. There are two main challenge in AV-ASR from the original ASR. One problem is how to extract visual features, the other problem is how to combine it with audio features. Visual speech information mainly on speaker's face. Extracting visual feature requires techniques from other field, such as face detection, head pose estimation, face feature localisation, tracking, feature extraction and other techniques to extract useful features from image with a face. Those techniques are prerequisite for incorporating AV-ASR in HCI. The second problem is how to make the combination of both modality produce better performance than using single modality. There are three type of fusion: combine audio and visual information at the feature level; combine audio and visual classifier scores at decision level; and a combination of both. \cite{potamianos2003recent} named two areas, visual front-end design and audiovisual fusion.

\section{Visual Front End}
A major problem in audiovisual automatic speech recognition is extracting visual feature from images. The process is to extract visual speech features from videos or a sequence of images. Generally, visual speech feature can be classified as three types: appearance feature, shape feature, combination of both\cite{potamianos2003recent}. Appearance feature usually means the image feature of Region of Interest. Image of (ROI) used to be directly used for training and classification. However, image data contains many noises and influenced by the lighting condition. Some techniques in computer vision are used to extract image features from Region of Interest. A good image descriptor is required for improving classification performance. Local Binary Pattern is a very good texture descriptor. Shape feature usually means contour of speaker's face, especially speaker's lips or including jaw and cheek. Shape feature usually means geometric-type features, there are many ways to describe shape feature of a face, such as statistical shape model or image moment descriptor of mouth, these model would be able to contain information of the height, width and other information of a mouth. Combined feature usually is the joint of both shape and appearance feature vectors or a model that include both features such as active appearance model.

In order to get appearance feature and shape feature or combined feature, there are some preprocessing steps should before extracting feature. Face detect, detecting the position of face, facial tracking, tracking facial feature points from frame to frame, face alignment, align facial feature points to a face, and ROI extracting technique extract features from ROI, these technique are all required for extracting visual speech features from videos. There are many methods for lip contour extraction: Snake by Kass et al(1998), Deformable Template by Yuille et al (1989), Active Shape Models by Cootes et al(1995), Active Appearance Models(AAMs) by Cootes et al(2000). The later three models are all called Parameterized Models, also in recent years, some parameterized model are extended and developed. some In order to remove head-pose, some deformable model of face are use to decompose the head points and remove head pose. If the head is not facing the front, appearance image of image need to be warped to frontal.
\subsection{Face Detection}
Face detection is to detect the location and size of single face or several faces in one digital image. There are several important aspects would influence face detection: background, head pose and lighting condition. There are two main approaches for face detection, one is non-statistical way, using traditional image processing techniques, the other is statistical approach, using statistical models. \cite{potamianos2003recent} uses traditional image processing techniques , such as color segmentation, edge detection, image thresholding, template matching or motion information. Some using statistical modelling such as Fisher Discriminant detector, Distance From Face Space(DFFS), Gaussian Mixture Classifier(GMM)and neural networks such as Artificial Neural Networks(ANN). Once a face is detected, use face alignment techniques to estimate the location of several facial feature around the face.
\subsection{Region of Interest}
Usually the choose of Region of Interest is depending on the purpose of the project. In AV-ASR it usually include large part of the lower face, such as the jaw, and cheeks or even the entire face \cite{potamianos2003recent}, as when people speak, the lower face would show some movements. In my project, the ROI only contains the gray-scale values of mouth region, which is scaled to $8 * 32$ size square region. \cite{potamianos2003recent} report that experiments shows that including jaw and cheeks was beneficial. As the tracker I use for tracking face does not include the jaw and cheek of the face, I have to just include the mouth.
\subsection{Visual Feature and Postprocessing}
Usually the image is not directly used for classification, because of the influence of noise and brightness. There are many descriptor can be used to represent a image and reduce the influence. In addition, the dimensionality of image vector is usually very large, it is not suitable for classification. The choice of visual feature is depends on the requirements of the project. The most popular descriptor for AV-ASR is texture feature descriptor. If the dimension of feature vector is too high, the common dimension reduction methods are traditional linear transforms. \cite{potamianos2003recent} gave some most commonly applied methods, Principle Component Analysis(PCA), discrete cosine transform(DCT), discrete wavelet transform, Hadamard and Haar transforms and a linear discriminant analysis based data projection. As the visual feature are extracted from variety of lighting condition and different face. These effects can be remedied by normalization, i.e. for each visual feature vector subtract it's mean and divide by the standard deviation. For different classification methods, some necessary steps may be needed before use feature vector to do classification.
\section{Facial Expression}
Facial expression could directly response to people's inner thoughts and feelings. Human face is major site for sensory input and outputs \cite{pantic2007machine}. Generally, face would show four kinds of signals: static facial signal, slow facial signals, artificial signals and rapid facial signals \cite{pantic2007machine}. Rapid facial signals underlie facial expressions. \cite{pantic2007machine} indicate that rapid facial signals generally show five types of messages: affective attitudinal states and moods which means emotions, emblems, manipulators, illustrators, regulators. For example, smile belongs to regulators and chewing belongs to manipulators. Automatic analysis of facial signals such as rapid facial signals have potential applications in many areas. Layers, security, police could be use automatic analysis of facial signal system to monitoring and interpreting human facial signal and gain important information. For example, monitoring human reaction during inquisition, inquisitor would be able to tell whether a person is lie or not.  Machine analysis of facial expression forms an important part of affective human-computer interface designs \cite{pantic2007machine}.  Research on machine analysis of facial expression mainly focuses on facial affect and facial muscle action detection\cite{pantic2007machine}. Technologies used in this area are face detection, facial feature extraction, facial muscle action detection and emotion recognition\cite{pantic2007machine}.
\subsection{Nonlinguistic Vocalization}
Non-linguistic vocalization is defined as brief, discrete, non-verbal expressions of affect in both face and voice \cite{petridis2011audiovisual} such as laugh, sighs, sob. Nonlinguistic Vocalization can be used to detect speaker's affective state and facilitating affect-sensitive HCI \cite{petridis2011audiovisual}. Many non-linguistic vocalization are able to show speaker's true reaction. \cite{petridis2011audiovisual} shows there are $0.8$ percent of time on laughing while talking.
\section{Audio-Visual Biometrics}
Biometrics recognition is using the utilization of physiological and behavioural characteristics for automatic person recognition \cite{aleksic2006audio}. Face and voice especially face are very personal biometric characteristics.Biometric Characteristics are often used for person identification and person verification. \cite{aleksic2006audio} indicates that traditional person recognition contains two types of method, one is knowledge-based, people using password, pin belong knowledge belong to that type, and the other is token-based,such as using a card with magnetic chip. Traditional methods are not good enough that they are either easily forgotten or stolen. Using biometric characteristics as personal identity could protect personal properties from being accessing by other person without worry about those problems. There are many different biometric characteristics could be used as person identity such as Iris, fingerprint, hand, signature. Although the performance of using face and speech is not as accurate as using Iris and finger print, but the sensor for face and speech recognition cost much less than Iris sensor. Face and speech recognizer can not be high secure bank, they are suitable for some less secure place, such as entrance check.